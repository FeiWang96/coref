{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions_of_cluster(dataset, cluster_id):\n",
    "    mentions = []\n",
    "    for mention in dataset:\n",
    "        if mention['coref_chain'] == cluster_id:\n",
    "            mentions.append(mention)\n",
    "\n",
    "    return mentions\n",
    "\n",
    "\n",
    "def get_all_chains(mentions):\n",
    "    clusters = {}\n",
    "    for mention_dic in mentions:\n",
    "        chain = mention_dic['coref_chain']\n",
    "        clusters[chain] = [] if chain not in clusters else clusters[chain]\n",
    "        clusters[chain].append(mention_dic)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_cluster_by_mention_num(clusters, num):\n",
    "    clusters_names = []\n",
    "    for cluster, doc_mention in clusters.items():\n",
    "        num_of_mentions = len(doc_mention)\n",
    "        if num_of_mentions == num:\n",
    "            clusters_names.append(cluster)\n",
    "\n",
    "    return clusters_names\n",
    "\n",
    "\n",
    "def get_gold_within_doc(mentions):\n",
    "    wd_cluster = {}\n",
    "    for mention in mentions:\n",
    "        chain = mention['coref_chain']\n",
    "        doc = mention['doc_id']\n",
    "        id_within_doc = chain + '_' + doc\n",
    "        wd_cluster[id_within_doc] = [] if id_within_doc not in wd_cluster else wd_cluster[id_within_doc]\n",
    "        wd_cluster[id_within_doc].append(mention)\n",
    "\n",
    "    return wd_cluster\n",
    "\n",
    "\n",
    "\n",
    "def get_metainfo(clusters):\n",
    "    \"\"\"\n",
    "    print num of mentions per clusters\n",
    "    :param clusters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for cluster, doc_mention in clusters.items():\n",
    "        num_of_mentions = len(doc_mention)\n",
    "        dic[num_of_mentions] = dic.get(num_of_mentions, 0) + 1\n",
    "\n",
    "    for length, num_of_clusters in sorted(dic.items()):\n",
    "        print(\"There are {} clusters with {} mentions\".format(num_of_clusters, length))\n",
    "\n",
    "    number = dic.values()\n",
    "    labels = dic.keys()\n",
    "\n",
    "    #get_pie_chart(number, labels)\n",
    "\n",
    "def extract_mention_text(cluster):\n",
    "    mentions = []\n",
    "    for mention in cluster:\n",
    "        mention.append(mention['MENTION_TEXT'])\n",
    "    return mentions\n",
    "\n",
    "\n",
    "def get_pie_chart(values, labels):\n",
    "    patches, texts = plt.pie(values, shadow=True, startangle=90)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def within_to_cross(within_doc_cluster):\n",
    "    cross_doc = {}\n",
    "    for within in within_doc_cluster:\n",
    "        name = within.split('_')[0]\n",
    "        if name != 'INTRA' and name != 'Singleton':\n",
    "            cross_doc[name] = [] if name not in cross_doc else cross_doc[name]\n",
    "            cross_doc[name].append(within)\n",
    "\n",
    "    return cross_doc\n",
    "\n",
    "\n",
    "def find_most_popular_word(clusters, within_doc_cluster):\n",
    "    words = {}\n",
    "    for cluster in clusters:\n",
    "        mentions = within_doc_cluster[cluster]\n",
    "        vocab = set()\n",
    "        for mention in mentions:\n",
    "            text = mention['MENTION_TEXT']\n",
    "            vocab.add(text)\n",
    "\n",
    "        for word in vocab:\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    most_word = max(words.items(), key=operator.itemgetter(1))\n",
    "    return most_word[0], most_word[1]/len(clusters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prob(within_doc_cluster):\n",
    "    cross_doc = within_to_cross(within_doc_cluster)\n",
    "    length = 0\n",
    "    prob = 0\n",
    "    for cluster, within in cross_doc.items():\n",
    "        word, coverage = find_most_popular_word(within, within_doc_cluster)\n",
    "        length += len(within)\n",
    "        prob += coverage * len(within)\n",
    "\n",
    "    return prob / length\n",
    "\n",
    "def get_distinct_mentions(mentions):\n",
    "    return list(set([m['tokens_str'] for m in mentions]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select data to explore (ECB+ or MEANTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'ecb_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8289 entity mentions\n",
      "6833 event mentions\n"
     ]
    }
   ],
   "source": [
    "with open(data + '/all_entity_gold_mentions.json', 'r') as f:\n",
    "    entity_mentions = json.load(f)\n",
    "\n",
    "with open(data + '/all_event_gold_mentions.json', 'r') as f:\n",
    "    event_mentions = json.load(f)\n",
    "    \n",
    "print('{} entity mentions'.format(len(entity_mentions)))\n",
    "print('{} event mentions'.format(len(event_mentions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Cross chains: 2741\n",
      "Event Within chains: 5496\n",
      "Entity Cross chains: 2221\n",
      "Entity Within chains: 5850\n"
     ]
    }
   ],
   "source": [
    "event_cross_clusters = get_all_chains(event_mentions)\n",
    "event_within_clusters = get_gold_within_doc(event_mentions)\n",
    "entity_cross_clusters = get_all_chains(entity_mentions)\n",
    "entity_within_clusters = get_gold_within_doc(entity_mentions)\n",
    "\n",
    "print('Event Cross chains: {}'.format(len(event_cross_clusters)))\n",
    "print('Event Within chains: {}'.format(len(event_within_clusters)))\n",
    "\n",
    "print('Entity Cross chains: {}'.format(len(entity_cross_clusters)))\n",
    "print('Entity Within chains: {}'.format(len(entity_within_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entity singleton: 1231\n",
      "Number of event singleton: 1775\n"
     ]
    }
   ],
   "source": [
    "entity_singleton = sum([1 for m in entity_mentions if m[\"coref_chain\"].startswith('Singleton')])\n",
    "event_singleton =  sum([1 for m in event_mentions if m[\"coref_chain\"].startswith('Singleton')])\n",
    "print('Number of entity singleton: {}'.format(entity_singleton))\n",
    "print('Number of event singleton: {}'.format(event_singleton))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore dominant mention method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def extract_dominant_mention(cluster_id, entity=True):\n",
    "    clusters = entity_cross_clusters if entity else event_cross_clusters\n",
    "    mentions = {}\n",
    "    different_docs = set()\n",
    "    for m in clusters[cluster_id]:\n",
    "        tokens = m['tokens_str']\n",
    "        mentions[tokens] = set() if tokens not in mentions else mentions[tokens]\n",
    "        mentions[tokens].add(m['doc_id'])\n",
    "        different_docs.add(m['doc_id'])\n",
    "    \n",
    "    for m, docs in mentions.items():\n",
    "        mentions[m] = len(docs)\n",
    "    \n",
    "    most_dominant = max(mentions.items(), key=operator.itemgetter(1))\n",
    "    return most_dominant[0], most_dominant[1], len(different_docs), most_dominant[1]/len(different_docs)\n",
    "\n",
    "\n",
    "def compute_statistics(entity=True, weight_avg=False):\n",
    "    clusters = entity_cross_clusters if entity else event_cross_clusters\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    exact = 0\n",
    "    dominant_mentions = []\n",
    "    num_of_docs = len(set(c for c, mentions in clusters.items() if len(mentions) > 1))\n",
    "    for cluster_id, mentions in clusters.items():\n",
    "        if len(mentions) > 1: #don't consider Singletons in this statistics\n",
    "            dominant, num, total_doc, percentage = extract_dominant_mention(cluster_id, entity)\n",
    "            dominant_mentions.append(dominant)\n",
    "            if weight_avg:\n",
    "                numerator += percentage * len(mentions)\n",
    "                denominator += len(mentions)\n",
    "            else:\n",
    "                numerator += percentage\n",
    "                denominator += 1\n",
    "            \n",
    "            if percentage == 1:\n",
    "                exact += 1\n",
    "    ambiguity = 1 - len(set(dominant_mentions)) / len(dominant_mentions)\n",
    "    return numerator/denominator, exact/num_of_docs, ambiguity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant mention: Tara Reid\n",
      "Appear in 16 documents on 18, percentage: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "dominant, num, total_doc, percentage = extract_dominant_mention('HUM16236184328979740')\n",
    "print(\"Dominant mention: {}\".format(dominant))\n",
    "print(\"Appear in {} documents on {}, percentage: {}\".format(num, total_doc, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: \n",
      "Number of document on average: 0.7049065438768808, \n",
      "Weight average: 0.6646222766185738, \n",
      "Ambiguity 0.18354430379746833\n",
      "All docs: 0.3556962025316456\n",
      "\n",
      "Event: \n",
      "Number of document on average: 0.7175639288110573, \n",
      "Weight average: 0.6596520022553044, \n",
      "Ambiguity 0.3268698060941828\n",
      "All docs: 0.36149584487534625\n"
     ]
    }
   ],
   "source": [
    "stat_entity, all_docs_entity, ambiguity_entity = compute_statistics(weight_avg=False)\n",
    "stat_entity_weight, all_docs_entity_weigh, ambiguity_entity_weight = compute_statistics(weight_avg=True)\n",
    "stat_event, all_docs_event, ambiguity_event = compute_statistics(entity=False, weight_avg=False)\n",
    "stat_event_weight, all_docs_event_weight, ambiguity_event_weight = compute_statistics(entity=False, weight_avg=True)\n",
    "\n",
    "print(\"Entity: \\nNumber of document on average: {}, \\nWeight average: {}, \\nAmbiguity {}\\nAll docs: {}\".\n",
    "      format(stat_entity, stat_entity_weight, ambiguity_entity, all_docs_entity))\n",
    "print()\n",
    "print(\"Event: \\nNumber of document on average: {}, \\nWeight average: {}, \\nAmbiguity {}\\nAll docs: {}\".\n",
    "      format(stat_event, stat_event_weight, ambiguity_event, all_docs_event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many clusters are actually across multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity - Number of clusters across multiple documents: 702 on 2221\n",
      "Event - Number of clusters across multiple documents: 669 on 2741\n"
     ]
    }
   ],
   "source": [
    "event_cross_doc_clusters = {}\n",
    "entity_cross_doc_clusters = {}\n",
    "\n",
    "for chain, mentions in entity_cross_clusters.items():\n",
    "    docs = list(set([m['doc_id'] for m in mentions]))\n",
    "    if len(docs) > 1:\n",
    "        entity_cross_doc_clusters[chain] = {'num_of_docs': len(docs), 'num_of_mentions': len(mentions)}\n",
    "\n",
    "for chain, mentions in event_cross_clusters.items():\n",
    "    docs = list(set([m['doc_id'] for m in mentions]))\n",
    "    if len(docs) > 1:\n",
    "        event_cross_doc_clusters[chain] = {'num_of_docs': len(docs), 'num_of_mentions': len(mentions)}\n",
    "\n",
    "print('Entity - Number of clusters across multiple documents: {} on {}'.\n",
    "      format(len(entity_cross_doc_clusters), len(entity_cross_clusters)))\n",
    "\n",
    "print('Event - Number of clusters across multiple documents: {} on {}'.\n",
    "      format(len(event_cross_doc_clusters), len(event_cross_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
