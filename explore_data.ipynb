{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions_of_cluster(dataset, cluster_id):\n",
    "    mentions = []\n",
    "    for mention in dataset:\n",
    "        if mention['coref_chain'] == cluster_id:\n",
    "            mentions.append(mention)\n",
    "\n",
    "    return mentions\n",
    "\n",
    "\n",
    "def get_all_chains(mentions):\n",
    "    clusters = {}\n",
    "    for mention_dic in mentions:\n",
    "        chain = mention_dic['coref_chain']\n",
    "        clusters[chain] = [] if chain not in clusters else clusters[chain]\n",
    "        clusters[chain].append(mention_dic)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_cluster_by_mention_num(clusters, num):\n",
    "    clusters_names = []\n",
    "    for cluster, doc_mention in clusters.items():\n",
    "        num_of_mentions = len(doc_mention)\n",
    "        if num_of_mentions == num:\n",
    "            clusters_names.append(cluster)\n",
    "\n",
    "    return clusters_names\n",
    "\n",
    "\n",
    "def get_gold_within_doc(mentions):\n",
    "    wd_cluster = {}\n",
    "    for mention in mentions:\n",
    "        chain = mention['coref_chain']\n",
    "        doc = mention['doc_id']\n",
    "        id_within_doc = chain + '_' + doc\n",
    "        wd_cluster[chain] = [] if id_within_doc not in wd_cluster else wd_cluster[chain]\n",
    "        wd_cluster[chain].append(mention)\n",
    "\n",
    "    return wd_cluster\n",
    "\n",
    "\n",
    "\n",
    "def get_metainfo(clusters):\n",
    "    \"\"\"\n",
    "    print num of mentions per clusters\n",
    "    :param clusters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for cluster, doc_mention in clusters.items():\n",
    "        num_of_mentions = len(doc_mention)\n",
    "        dic[num_of_mentions] = dic.get(num_of_mentions, 0) + 1\n",
    "\n",
    "    for length, num_of_clusters in sorted(dic.items()):\n",
    "        print(\"There are {} clusters with {} mentions\".format(num_of_clusters, length))\n",
    "\n",
    "    number = dic.values()\n",
    "    labels = dic.keys()\n",
    "\n",
    "    #get_pie_chart(number, labels)\n",
    "\n",
    "def extract_mention_text(cluster):\n",
    "    mentions = []\n",
    "    for mention in cluster:\n",
    "        mention.append(mention['MENTION_TEXT'])\n",
    "    return mentions\n",
    "\n",
    "\n",
    "def get_pie_chart(values, labels):\n",
    "    patches, texts = plt.pie(values, shadow=True, startangle=90)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def within_to_cross(within_doc_cluster):\n",
    "    cross_doc = {}\n",
    "    for within in within_doc_cluster:\n",
    "        name = within.split('_')[0]\n",
    "        if name != 'INTRA' and name != 'Singleton':\n",
    "            cross_doc[name] = [] if name not in cross_doc else cross_doc[name]\n",
    "            cross_doc[name].append(within)\n",
    "\n",
    "    return cross_doc\n",
    "\n",
    "\n",
    "def find_most_popular_word(clusters, within_doc_cluster):\n",
    "    words = {}\n",
    "    for cluster in clusters:\n",
    "        mentions = within_doc_cluster[cluster]\n",
    "        vocab = set()\n",
    "        for mention in mentions:\n",
    "            text = mention['MENTION_TEXT']\n",
    "            vocab.add(text)\n",
    "\n",
    "        for word in vocab:\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    most_word = max(words.items(), key=operator.itemgetter(1))\n",
    "    return most_word[0], most_word[1]/len(clusters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prob(within_doc_cluster):\n",
    "    cross_doc = within_to_cross(within_doc_cluster)\n",
    "    length = 0\n",
    "    prob = 0\n",
    "    for cluster, within in cross_doc.items():\n",
    "        word, coverage = find_most_popular_word(within, within_doc_cluster)\n",
    "        length += len(within)\n",
    "        prob += coverage * len(within)\n",
    "\n",
    "    return prob / length\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data to explore (ECB+ or MEANTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'meantime_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2866 entity mentions\n",
      "2107 event mentions\n"
     ]
    }
   ],
   "source": [
    "with open(data + '/all_entity_gold_mentions.json', 'r') as f:\n",
    "    entity_mentions = json.load(f)\n",
    "\n",
    "with open(data + '/all_event_gold_mentions.json', 'r') as f:\n",
    "    event_mentions = json.load(f)\n",
    "    \n",
    "print('{} entity mentions'.format(len(entity_mentions)))\n",
    "print('{} event mentions'.format(len(event_mentions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entity chains: 873\n",
      "Number of event chains: 1892\n"
     ]
    }
   ],
   "source": [
    "entity_cross_clusters = get_all_chains(entity_mentions)\n",
    "event_cross_clusters = get_all_chains(event_mentions)\n",
    "print('Number of entity chains: {}'.format(len(entity_cross_clusters)))\n",
    "print('Number of event chains: {}'.format(len(event_cross_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entity without instance_id: 122\n",
      "Number of event without instance_id: 1315\n"
     ]
    }
   ],
   "source": [
    "entity_singleton = sum([1 for m in entity_mentions if m[\"coref_chain\"].startswith('Singleton')])\n",
    "event_singleton =  sum([1 for m in event_mentions if m[\"coref_chain\"].startswith('Singleton')])\n",
    "print('Number of entity without instance_id: {}'.format(entity_singleton))\n",
    "print('Number of event without instance_id: {}'.format(event_singleton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = entity_mentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cluster_desc', 'coref_chain', 'doc_id', 'event_entity', 'full_sentence', 'is_pronoun', 'left_sentence', 'm_id', 'mention_type', 'right_sentence', 'sent_id', 'tokens_ids', 'tokens_str', 'topic'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {}\n",
    "for m in entity_mentions:\n",
    "    m_type = m['mention_type']\n",
    "    types[m_type] = types.get(m_type, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRO': 750, 'ORG': 903, 'PER': 357, 'LOC': 374, 'FIN': 360, '': 122}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2744"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "750+903+357+374+360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor = {}\n",
    "for m in entity_mentions:\n",
    "    if m[\"coref_chain\"].startswith('Singleton_'):\n",
    "        desc = m[\"coref_chain\"]\n",
    "        descriptor[desc] = descriptor.get(desc, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for m in entity_mentions if m['coref_chain'].startswith('Singleton')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event chains: 1185\n",
      "Entity chains: 791\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "event = set()\n",
    "entity = set()\n",
    "with open('datasets/meantime_newsreader_english_oct15/list_instances.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        if line[1] == 'EVENT':\n",
    "            event.add(line[0])\n",
    "        else:\n",
    "            entity.add(line[0])\n",
    "\n",
    "print('Event chains: {}'.format(len(event)))\n",
    "print('Entity chains: {}'.format(len(entity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n"
     ]
    }
   ],
   "source": [
    "chains_from_mentions = set()\n",
    "for m in entity_mentions:\n",
    "    chains_from_mentions.add(m['coref_chain'])\n",
    "print(len(chains_from_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO19274271812050726\n",
      "FIN36825527747670669\n",
      "FIN36825527589495792\n",
      "PRO27737916609991057\n",
      "PRO47037235314943369\n",
      "PRO36042008717728899\n",
      "PRO56150570102047852\n",
      "LOC39494944810590513\n",
      "FIN27982806499965672\n",
      "PRO55898465939579653\n",
      "#ID\n",
      "ORG48295538177040363\n",
      "ORG45374187885304059\n",
      "PRO39503428423512753\n",
      "PRO39493678270400934\n",
      "PRO27737342834503629\n",
      "PRO56092201179325546\n",
      "ORG27744131024879930\n",
      "FIN27982311252830625\n",
      "PRO27738187661870589\n",
      "PRO56087535324635302\n",
      "PRO55741017831652172\n",
      "ORG19093168857338940\n",
      "PRO47041690110532455\n",
      "ORG27980955745626409\n",
      "PER25304436893090468\n",
      "PRO39493700907346891\n",
      "ORG27733658216837514\n",
      "ORG27980020499213139\n",
      "ORG27984880689122584\n",
      "PRO47186532201508230\n",
      "PRO41488049088478591\n",
      "ORG35073822103445135\n",
      "PRO27573052674952028\n",
      "FIN44153078778154325\n",
      "PRO36276501830687088\n",
      "PRO27733281450580678\n",
      "PRO47184870591447505\n",
      "PRO27734230348663983\n",
      "ORG19093169632160927\n",
      "ORG19274271618816482\n"
     ]
    }
   ],
   "source": [
    "for c in entity:\n",
    "    if c not in chains_from_mentions:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_desc': 'European Central Bank', 'coref_chain': 'ORG44081990077682597', 'doc_id': '113278_Markets_rally_as_world_central_banks_infuse_cash.xml', 'event_entity': 'entity', 'full_sentence': 'Totaling US$ 180 billion , the Federal Reserve arranged to increase existing swap lines with the European Central Bank from US$ 55 billion to US$ 110 billion , and with the Swiss National Bank from US$ 12 billion to US$ 27 billion .', 'is_pronoun': False, 'left_sentence': 'Totaling US$ 180 billion , the Federal Reserve arranged to increase existing swap lines with the', 'm_id': '49', 'mention_type': 'ORG', 'right_sentence': 'from US$ 55 billion to US$ 110 billion , and with the Swiss National Bank from US$ 12 billion to US$ 27 billion .', 'sent_id': '5', 'tokens_ids': [138, 139, 140], 'tokens_str': 'European Central Bank', 'topic': 'stock'}\n",
      "{'cluster_desc': 'European Central Bank', 'coref_chain': 'ORG44081990077682597', 'doc_id': '113278_Markets_rally_as_world_central_banks_infuse_cash.xml', 'event_entity': 'entity', 'full_sentence': 'At 0800 UTC , the Bank of Canada , the Bank of England , the European Central Bank ( ECB ) , the United States Federal Reserve , the Bank of Japan , and the Swiss National Bank jointly announced that they would be working together to provide reciprocal credit arrangements to institutions facing a financial crush from the U.S. dollar .', 'is_pronoun': False, 'left_sentence': 'At 0800 UTC , the Bank of Canada , the Bank of England , the European Central Bank', 'm_id': '64', 'mention_type': 'ORG', 'right_sentence': ', the United States Federal Reserve , the Bank of Japan , and the Swiss National Bank jointly announced that they would be working together to provide reciprocal credit arrangements to institutions facing a financial crush from the U.S. dollar .', 'sent_id': '3', 'tokens_ids': [54, 55, 56], 'tokens_str': '( ECB )', 'topic': 'stock'}\n",
      "{'cluster_desc': 'European Central Bank', 'coref_chain': 'ORG44081990077682597', 'doc_id': '113278_Markets_rally_as_world_central_banks_infuse_cash.xml', 'event_entity': 'entity', 'full_sentence': 'At 0800 UTC , the Bank of Canada , the Bank of England , the European Central Bank ( ECB ) , the United States Federal Reserve , the Bank of Japan , and the Swiss National Bank jointly announced that they would be working together to provide reciprocal credit arrangements to institutions facing a financial crush from the U.S. dollar .', 'is_pronoun': False, 'left_sentence': 'At 0800 UTC , the Bank of Canada , the Bank of England ,', 'm_id': '16', 'mention_type': 'ORG', 'right_sentence': '( ECB ) , the United States Federal Reserve , the Bank of Japan , and the Swiss National Bank jointly announced that they would be working together to provide reciprocal credit arrangements to institutions facing a financial crush from the U.S. dollar .', 'sent_id': '3', 'tokens_ids': [50, 51, 52, 53], 'tokens_str': 'the European Central Bank', 'topic': 'stock'}\n"
     ]
    }
   ],
   "source": [
    "for m in entity_mentions:\n",
    "    if m['coref_chain'] == 'ORG44081990077682597':\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
