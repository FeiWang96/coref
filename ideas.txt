Annotation incoherence:

Topic 1:
1_1ecb: 'another star in rehab' (sentence 0) star and rehab do not coref with any entity
1_2ecb: 'Promises Treatment Center' (sentence 1) is missing in the annotation
	'American Pie's star' (sentence 1) does not coref to Tara Reid
	'Jack Ketsoyan' was not annotated 
	'Terry White'(1_12ecbplus.xml) was not annotated on the cluster 
	'TMZ' (1_9ecbplus sentence 3) was not annotated
1_4ecb: 'Promises' was not annotated
and more...

1_2ecb '... checked herself into ...'
m_id = 29: checked herself
m_id = 30: checked into
two different mentions ?

another ambiguity point: the pinup girl Williams. Intuitively, i have thought there are two or three mentions : pinup girl (or pinup and girl separately) and Williams

ambiguity (good point):
same lemma for both entity and event cluster


Within document:
1. Within sentence: who, which, that, who, where, there... --> can be easily solved thanks to dependency parse or SRL
2. Cross sentences: pronoun, hypernym, position (actress, attorney, starlet), substring, only first or last name of a proper noun ...
example: 'the same place', can be everything
3. Cross document: in the majority, the mention with the most occurrence will appear in all document, maybe can we score cluster using that ?

--> within document is more complicated than cross document because of pronouns and aliases.
if we do well the within document, we will be able to do well the cross too representing the cluster by the mention with the more occurrence.
mention includes also arguments and features...


events cluster: event can be either a verb or a noun

Idea: methodological method that consists to build a hierarchy of clusters, first cluster at the sentence level and then extend to other sentences and to other documents.

Cross document:

-good representation of the cluster: 'head-word' of the cluster, word excepts pronoun appear most, location, time and participants
single vector to represent all the cluster with all informations.

what about entity cluster representation ? except the pronoun, the word with the most occurrence, maybe one or two another ones. think about nouns compounds like prime minister for embeddings (ELMO, BERT...)

entity cluster can help event cluster but the opposite is wrong, because it will disturb when the same entity participates on different events.
but in the ECB corpus, it looks like same entity does no participate on different events
see example of judge HUM15830027488237317
judge_who_required_treatment HUM15830027488237317
a_judge HUM16002409593260103
judge_allowing_cliffside_move HUM15737056436705907
judge_dabney HUM15986365101070461
--> event cluster representation may contains information about entity cluster


Look at the data:


Entity: composed noun -- ex: electronics store
	plural vs sing are not in the same cluster (facility, facilities) -- embeddings won't solve it
	'today': look at the time of the document
	within sentence
	bosom of the facility -- bosom is considered mention and cluster ?
	judge: 4 clusters with the word judge but different entities
	HUM15986365101070461: 3 times the exact name of the judge James R . Dabney
	and 3 times only the word 'judge', how can we known they coref ?
	1.Heller told a judge ... that Lohan had checked in to the Morningside
	Jay Heller told LA Superior Court Judge James R . Dabney that Lohan , 26 , had 		checked into Morningside -- same event 
	2. A Superior Court judge
	3. there is another one, hard to identify even by humans, can be a mistake
	
	other words: words around, context

	HUM16253626692271271, looks strange
	
	
Event:	despite the separable tagging to event and negative event, there are some examples 	of negatives examples tagged as event. 
	ex: file 1_5ecbplus.xml sentence 8 'she never entered Morningside'

	advantage: good diversity of language: for example regarding the event 'check in a rehab', even in the ECB+, there are 3 different events in 3 different rehabs (Morningside,  BettyFord and cliffside)

 
within document: pairwise
cross document: cluster vector

contextual word embeddings is supposed to take pronouns coreference
maybe take the upper layer
span representation: first, last, product-wise, minus
maybe unsupervised ? not enough data to learn a model to coref
replace pronoun (he, she, it...) by the coref NP,

getting two within doc cluster, how to represent them to get high similarity when they corefer and low when they not
what about event ?
can we learn cluster representation by self attention and so on ?
positive data: within doc cluster refer
negative data: within doc cluster of the same topic or even the same subtopic but not the same cluster
we can take k negative for one positive (like word2vec) 

bag of mention representation, we can learn a classifier


singleton on all the corpus
singleton in one doc but coref with other within doc cluster (singleton or not)
within doc cluster not singleton 




after one merge between two clusters, retrain all the model ? it won't be very different
find a solution to explore an another solution







MEANTIME:

entities are divided into Person, Organizations, Locations, Product and Financial
entity mentions, event mentions, time expressions, and numerical expressions are annotated separately
Check the correctness of the head value for each mention

